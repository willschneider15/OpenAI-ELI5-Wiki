# Tokenization

Tokenization is like sorting things into different boxes. 

* In computer science, we use tokenization to separate words, phrases, or symbols into their own "boxes".
* Each of these boxes is called a token.
* We do this so we can easily analyze them separately and perform specific actions on them.
* For example, if we wanted to count how many times a certain word appears in a text, we would first need to separate all of the words into tokens.
* This also helps with data processing and storage, since we can store the tokens separately rather than as one long string of text.
* Tokenization can also be used in other ways, like in programming languages where we use symbols to give different commands, and we can separate these commands into individual tokens to help the computer understand exactly what it needs to do. 
* Overall, tokenization is a helpful way to break down larger pieces of information into smaller, more manageable parts.
