# Big-O

Big-O notation is a way of describing how long it takes for a computer program to execute. Its main purpose is to help us understand how efficient our code is. 

Here are some key points to remember:

* It measures the **worst-case scenario** time complexity of an algorithm.
* The notation uses the letter O, followed by a function that describes how the running time of the algorithm grows.
* The function used is typically based on the size of the input data, denoted as n.
* For example, O(1) means that the program takes constant time to execute, no matter the size of the input data.
* On the other hand, O(n) means that the program's running time grows linearly with the size of input data.
* As the letter 'n' grows larger, the complexity of the program can grow exponentially, represented as O(2^n), O(n!), etc. 

In summary, big-O notation helps us measure the efficiency of our algorithms, as well as understand how they will perform as input data grows larger. By analyzing algorithms in this way, we can make informed decisions about which algorithm to use in a given situation.
